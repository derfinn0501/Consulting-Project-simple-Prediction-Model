{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a33a588",
   "metadata": {},
   "source": [
    "# PANIC Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd84661",
   "metadata": {},
   "source": [
    "Prototypical additive neural network for interpretable classification that integrates 3D image and tabular data.\n",
    "\n",
    "PANIC consists of one neural net for 3D image data, one neural net for each tabular feature, and combines their outputs via summation to yield the final prediction.\n",
    "\n",
    "Therefore Panic can be seen as GAM extended by functions that measure similarities between an input image and a set of class specific prototypes:\n",
    "\n",
    "$$\n",
    "p(c \\mid x_1, \\ldots, x_N, \\mathcal{I}) = \\text{softmax}(\\mu^c) \\\\ \n",
    "\\quad \\mu^c = \\beta_0^c + \\sum_{n=1}^N f_n^c(x_n) + \\sum_{k=1}^K g_k^c(\\mathcal{I})\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "\n",
    "- p(c) is the probability that an individual belongs to class c given the data\n",
    "- $\\mu^c$ is a GAM where $f_n^c(x_n)$ is the class-specific output for feature n and $g_k^c$ is the similarity of one image to the k-th prototype of class c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9554df60",
   "metadata": {},
   "source": [
    "## Tabular Model $f_n^c(x_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6af79e",
   "metadata": {},
   "source": [
    "If $x_n$ is continous $f_n^c(x_n)$ is a a multi-layer perceptron (MLP) [1]. \\\n",
    "If $x_n$  is discrete it is estimated linear (step function)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6628e5",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a4c149",
   "metadata": {},
   "source": [
    "Following [1], $\\ell_2$ penalty is applied on the outpuits of $f_n^c(x_n)$.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{Tab}}(x_1, \\ldots, x_n) \\;=\\; \\frac{1}{C} \\sum_{c=1}^C \\sum_{n=1}^N \\Big[ f_n^c(x_n) \\Big]^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacc67c0",
   "metadata": {},
   "source": [
    "## Image Model $g_k^c$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41198412",
   "metadata": {},
   "source": [
    "Multiple types of prototypes are possible: \n",
    "-\tProtoPNet can learn a single prototype for the disease\n",
    "--> Might not be sufficient as a disease might manifest in multiple regions (e.g. the hippocampus appears in the left and right hemisphere)\n",
    "-\tDeformable ProtoPNet  allows multiple fine-grained prototypical parts to extract prototypes, but is bound to a fixed number of prototypical parts that represent a prototype\n",
    "-\tXProtoNet [11] overcomes this limitation by defining prototypes based on attention masks rather than patches; it has been applied for lung disease classification from radiographic images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9bd6fb",
   "metadata": {},
   "source": [
    "### XProtoNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cafd042",
   "metadata": {},
   "source": [
    "An image is classified based on the cosine similarity between a latent feature vector $z^{pc}_k$ and learned class-specific prototypes $p^c$.\n",
    "\n",
    "$$\n",
    "g_k^c(\\mathcal{I}) = \\text{sim}(p_k^c, z_{p_k}^c) \n",
    "= \\frac{p_k^c \\cdot z_{p_k}^c}{\\lVert p_k^c \\rVert \\, \\lVert z_{p_k}^c \\rVert}\n",
    "\\tag{3}\n",
    "$$\n",
    "\n",
    "- Measures the **angle** between two vectors, ignoring their magnitude.\n",
    "- Range: $[-1, 1]$\n",
    "  - $1$: vectors perfectly aligned (high similarity)  \n",
    "  - $0$: orthogonal (no similarity)  \n",
    "  - $-1$: opposite directions (high dissimilarity)  \n",
    "- Used to decide how close a latent feature vector is to a class prototype.\n",
    "- The denominator removes the effect of vector length.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a91c389",
   "metadata": {},
   "source": [
    "#### Latent Feature Vector $z^{pc}_k$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a5dd67",
   "metadata": {},
   "source": [
    "A latent feature vector $z^{pc}_k$ is obtained by passing an image $\\mathcal{I}$ into a CNN backbone\n",
    "\n",
    "$$\n",
    "U : \\mathbb{R}^{1 \\times H \\times D \\times W} \\;\\to\\; \\mathbb{R}^{R \\times H' \\times D' \\times W'}\n",
    "$$  \n",
    "\n",
    "where $R$ is the number of output channels. The result is passed into two separate modules:  \n",
    "\n",
    "1. **Feature extractor**  \n",
    "   $$\n",
    "   V : \\mathbb{R}^{R \\times H' \\times D' \\times W'} \\;\\to\\; \\mathbb{R}^{L \\times H' \\times D' \\times W'}\n",
    "   $$  \n",
    "   which maps the feature map to the dimensionality of the prototype space $L$.  \n",
    "\n",
    "2. **Occurrence module**  \n",
    "   $$\n",
    "   O^c : \\mathbb{R}^{R \\times H' \\times D' \\times W'} \\;\\to\\; \\mathbb{R}^{K \\times H' \\times D' \\times W'}\n",
    "   $$  \n",
    "   which produces $K$ class-specific attention masks.  \n",
    "\n",
    "Finally, the latent feature vector is defined as  \n",
    "\n",
    "$$\n",
    "z^{pc}_k = \\text{GAP}\\!\\left[ \\; \\sigma\\!\\big(O^c(U(\\mathcal{I}))_k\\big) \\;\\odot\\; \\text{softplus}\\!\\big(V(U(\\mathcal{I}))\\big) \\;\\right],\n",
    "\\tag{4}\n",
    "$$  \n",
    "\n",
    "where  \n",
    "- $\\odot$ denotes the Hadamard product,  \n",
    "- $\\sigma$ is the sigmoid function,  \n",
    "- **GAP** is global average pooling.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8182a6",
   "metadata": {},
   "source": [
    "**Intuition of each step:**\n",
    "\n",
    "- **Backbone $U$:** extracts general CNN feature maps from the input image $\\mathcal{I}$.\n",
    "- **Feature extractor $V$:** projects these features into the **prototype space** of dimension $L$.\n",
    "- **Occurrence module $O^c$:** generates $K$ class-specific attention masks that highlight *where* prototypes are present in the image.  \n",
    "  - Useful even if anatomy is fixed: abnormalities vary in location, irrelevant regions are suppressed, and robustness is added across datasets.\n",
    "- **Sigmoid + Softplus:**  \n",
    "  - Sigmoid $(\\sigma)$: squashes mask values to $[0,1]$ (attention weights).  \n",
    "  - Softplus: ensures feature activations are positive and smoothly scaled.\n",
    "- **Hadamard product ($\\odot$):** combines the attention mask with the feature map elementwise, keeping only the attended regions.\n",
    "- **Global Average Pooling (GAP):** aggregates the weighted features into a single latent prototype vector $z^{pc}_k$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947f538a",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d168d1",
   "metadata": {},
   "source": [
    "1. Agarwal, R., Melnick, L., Frosst, N., et al.: Neural additive models: interpretable\n",
    "machine learning with neural nets. In: NeurIPS, vol. 34, pp. 4699–4711 (2021)\n",
    "\n",
    "\n",
    "\n",
    "11. Kim, E., Kim, S., Seo, M., Yoon, S.: XProtoNet: diagnosis in chest radiography\n",
    "with global and local explanations. In: CVPR, pp. 15719–15728 (2021)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
